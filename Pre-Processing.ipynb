{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Title**: Detecting Insults in Social Commentary"
      ],
      "metadata": {
        "id": "rXVxR-Xs-VdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Description:**\n",
        "In today's digital age, online discussions and social media have become an integral part of our lives. However, with the convenience of online communication comes the challenge of moderating and ensuring respectful discourse. This project aims to address a critical issue: **detecting and identifying insulting comments in social commentary**.\n",
        "\n",
        "The project focuses on the task of identifying comments that are intended to insult or demean participants in a conversation. These comments may contain profanity, offensive language, racial slurs, or other forms of disrespect. It's **important to note** that we are specifically interested in comments that target participants of the discussion, not public figures or celebrities."
      ],
      "metadata": {
        "id": "KyPeGBa2_P0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project Objectives**\n",
        "\n",
        "1. **Classifier Development:** The primary objective is to build a machine learning classifier that can accurately predict whether a given comment is insulting. This classifier should assign a probability score to each comment, indicating the likelihood of it being an insult.\n",
        "2. **Accuracy Priority:** Maximize accuracy while minimizing false positives and false negatives, achieving a balanced model.\n",
        "\n",
        "3. **Real-time Detection:** Develop a near-real-time model for automatic insult detection in online conversations.\n",
        "\n",
        "4. **Generalization:** Ensure the model handles diverse insults, including explicit and subtle forms, promoting generalizability.\n",
        "\n",
        "5. **Data Privacy:** Adhere to strict data protection standards, using comments solely for moderation purposes.\n",
        "\n",
        "6. **Scalable Solution:** Create a scalable system to process high volumes of user-generated content efficiently.\n",
        "\n",
        "7. **Ethical Compliance:** Address ethical concerns, including potential biases and fairness in the insult detection system.\n",
        "\n",
        "8. **Collaboration:** Collaborate with online platforms and communities for system implementation and refinement."
      ],
      "metadata": {
        "id": "03Vew3xK_k4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Understanding**\n",
        "\n",
        "To begin the project, let's analyze the available data. We'll create dataframes with the necessary input files, explore the data, and describe all the columns. Understanding the data is essential for developing an effective insult detection model."
      ],
      "metadata": {
        "id": "X6-uZEo5Ad__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "LrTcON5FCYNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "verification_set = pd.read_csv('/impermium_verification_set.csv')\n",
        "verification_labels = pd.read_csv('/impermium_verification_labels.csv')\n",
        "test = pd.read_csv('/test.csv')\n",
        "test_with_solutions = pd.read_csv('/test_with_solutions.csv')\n",
        "train = pd.read_csv('/train.csv')\n",
        "sample_submission_null = pd.read_csv('/sample_submission_null.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "Is9ZSfnMEMk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Explanations**\n",
        "\n",
        "#### **Verification Set (`verification_set`)**:\n",
        "This dataset is used for verification purposes during model development and testing.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5yO6ydp_K8ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 rows of the verificationSet DataFrame\n",
        "print(\"verification_set:\")\n",
        "print(verification_set.head(10))"
      ],
      "metadata": {
        "id": "zpjg76DmEaVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Verification Labels (`verification_labels`)**:\n",
        "This dataset provides labels for the 'verification_set,' with '0' indicating non-insulting and '1' indicating insulting comments.\n"
      ],
      "metadata": {
        "id": "7BAoZXyrNZMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 rows of the verification labels\n",
        "print(\"verification_labels:\")\n",
        "print(verification_labels.head(10))"
      ],
      "metadata": {
        "id": "ZaegVRPKE1yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test (`test`)**:\n",
        "The main test dataset used to make predictions with the trained machine learning model."
      ],
      "metadata": {
        "id": "wJGpVAnANZzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 rows of the test data frame\n",
        "print(\"test:\")\n",
        "print(test.head(10))"
      ],
      "metadata": {
        "id": "PHHvAn7cE8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test with Solutions (`test_with_solutions`):**\n",
        "Similar to the \"test\" dataset, but includes the ground truth or correct answers (solutions) for evaluation purposes. Used to assess model performance after predictions are made."
      ],
      "metadata": {
        "id": "dbxC1Kj3NakC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 rows of the test_with_solutions data frame\n",
        "print(\"test_with_solutions:\")\n",
        "print(test_with_solutions.head(10))"
      ],
      "metadata": {
        "id": "97MBNgVFIiYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train (`train`)**:\n",
        "The training dataset used for training the machine learning model. It contains labeled data (comments with known insult labels) to develop and train the insult detection model."
      ],
      "metadata": {
        "id": "rYhf5HZiNbBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 rows of the train data frame\n",
        "print(\"train:\")\n",
        "print(train.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEghYg3YIscE",
        "outputId": "32c5d7c8-744f-4940-b270-7a8ceef96365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:\n",
            "   Insult             Date                                            Comment\n",
            "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
            "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
            "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
            "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
            "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
            "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...\n",
            "6       0  20120503012628Z                      \"Yeah and where are you now?\"\n",
            "7       1              NaN  \"shut the fuck up. you and the rest of your fa...\n",
            "8       1  20120502173553Z  \"Either you are fake or extremely stupid...may...\n",
            "9       1  20120620160512Z  \"That you are an idiot who understands neither...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sample Submission Null (`sample_submission_null`)**:\n",
        "Provides a sample submission format to guide participants in formatting their predictions correctly when submitting to the competition.\n"
      ],
      "metadata": {
        "id": "Et0J3hd9NbmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 rows of the sample_submission_null data frame\n",
        "print(\"sample_submission_null:\")\n",
        "print(sample_submission_null.head(10))"
      ],
      "metadata": {
        "id": "HXxp2cbhI0jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next step, we will focus solely on the 'train' dataset as we prepare to preprocess the data. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing"
      ],
      "metadata": {
        "id": "PiDIyCJxOK6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk                                # Python library for NLP\n",
        "import matplotlib.pyplot as plt            # library for visualization"
      ],
      "metadata": {
        "id": "HWGxvfkKOMhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We extract positive and negative comments to two separate data frames."
      ],
      "metadata": {
        "id": "t4xBInQXP_br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract positive comments (Insult = 1) and select the 'Comment' column\n",
        "positive_comments = train[train['Insult'] == 1]['Comment']\n",
        "\n",
        "# Extract negative comments (Insult = 0) and select the 'Comment' column\n",
        "negative_comments = train[train['Insult'] == 0]['Comment']"
      ],
      "metadata": {
        "id": "SmhLhOTfP8-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(positive_comments))\n",
        "print(len(negative_comments))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgEmvjvrQKot",
        "outputId": "368f159c-b87c-4813-a22c-00684605dbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1049\n",
            "2898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will develop a text preprocessing function that will be applied to the comment text."
      ],
      "metadata": {
        "id": "7q__xgiESJ3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "metadata": {
        "id": "AqrZbEHeTh8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove hyperlinks, Twitter marks and styles"
      ],
      "metadata": {
        "id": "0_ykCluZTukF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing positive comments containing specific characters like '@' and 'https' to take as a sample\n",
        "print(positive_comments[positive_comments.str.contains('@|https', case=False)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_121PIewTxZg",
        "outputId": "ca6049da-ad7f-441c-d669-f8d90afdef44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "248     \"yeah I'm pathetic but your the idiot going ar...\n",
            "761     \"@peter8888,\\n\\nYou showed your true colors. I...\n",
            "835     \"Bitch, you replied to my comment, you stupid ...\n",
            "1009    \"Yo\\xa0@LukeEmery:disqus\\xa0you must be too du...\n",
            "1078    \"@berethor099 Go ahead and try, dude. Go - fuc...\n",
            "1145    \"@Pickle\\n\\nYou look like Brian Scalabrine's b...\n",
            "1428    \"@TeeBooWa\\n\\xa0HAH YOUR'E ONLY 14? FPFF AND Y...\n",
            "1585    \"Why don't you take your pathetic intimidation...\n",
            "1612               \"@justin_mia Perkins is a knucklehead\"\n",
            "1917    \"YOU ARE THE REAL @SSHO LE AND I HOPE PEOPLE O...\n",
            "2029    \"@Hoss \\xa0\\xa0@JenBroflovski\\xa0fuck you you ...\n",
            "2076               \"@caljb7 i'm on AAC then ya dumb fuck\"\n",
            "2105    \"@Enundr\\xa0lol.....You sir, are a jackass. wi...\n",
            "2456                   \"@FUCK:disqus\\xa0YOU CNN FAGGOTS.\"\n",
            "3079    \"@cnn-fcbba858f167b1594a66777bca:disqus \\n\\nYo...\n",
            "3100    \"@Bexxcc\\xa0\\xa0@bubzsucz\\n\\xa0YOU'RE THE SICK...\n",
            "3174    \"@lazerbyte Shut the fuck up -_- so where do y...\n",
            "3279    \"not funny jack@$$. you are a total db! friggi...\n",
            "3282    \"@RN506:disqus Read through the comments on th...\n",
            "3295    \"@ ede444 and that's all you've got to say is ...\n",
            "3306    \"Excellent command of the English language! On...\n",
            "3414         \"@bizora2:disqus you are a pathetic nobody.\"\n",
            "3677    \"@adamomars\\xa0\\xa0@Dieofnv\\xa0the very last t...\n",
            "3924    \"@Crissa:disqus LaRaza (The Race), NBP. Nation...\n",
            "Name: Comment, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We take a sample comment\n",
        "comment = positive_comments[1078]\n",
        "print(comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlDMHcq5Vlrn",
        "outputId": "fc3eb515-1166-4894-c4eb-2448c8d986be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"@berethor099 Go ahead and try, dude. Go - fucking - ahead.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We download the stopwords from Natural Language Toolkit\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7BbW9crUCY1",
        "outputId": "3876c95a-953b-47ce-8896-9f3f03e7ffde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[92m' + comment)\n",
        "print('\\033[94m')\n",
        "\n",
        "# remove the \"\" from the end and start of the sentences\n",
        "comment1 = comment.strip('\"')\n",
        "\n",
        "# remove hyperlinks\n",
        "comment1 = re.sub(r'https?://[^\\s\\n\\r]+', '', comment1)\n",
        "\n",
        "# only removing the hash # sign from the word\n",
        "comment1 = re.sub(r'#', '', comment1)\n",
        "\n",
        "print(comment1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkjsqiYxUhl1",
        "outputId": "9e29ec2f-7618-4a1f-ed11-7b2f50347347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m\"@berethor099 Go ahead and try, dude. Go - fucking - ahead.\"\n",
            "\u001b[94m\n",
            "@berethor099 Go ahead and try, dude. Go - fucking - ahead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the string"
      ],
      "metadata": {
        "id": "6tTD6KiIYCXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[92m' + comment1)\n",
        "print('\\033[94m')\n",
        "\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "comment_tokens = tokenizer.tokenize(comment1)\n",
        "\n",
        "print('Tokenized string:')\n",
        "print(comment_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtM17DnrYBSD",
        "outputId": "3907dccb-488f-499b-f428-59c409c23ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m@berethor099 Go ahead and try, dude. Go - fucking - ahead.\n",
            "\u001b[94m\n",
            "Tokenized string:\n",
            "['go', 'ahead', 'and', 'try', ',', 'dude', '.', 'go', '-', 'fucking', '-', 'ahead', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stop words and punctuations"
      ],
      "metadata": {
        "id": "MuXmmGcSYma7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4B0K1XsYUEX",
        "outputId": "4672bd50-df31-4fe4-c183-dc859f961be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might need to customize the stop words list for our applications. Since the model should diffrenciate between insults to users of the forum and others."
      ],
      "metadata": {
        "id": "Due5IXoJZHUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[92m')\n",
        "print(comment_tokens)\n",
        "print('\\033[94m')\n",
        "comment_clean = []\n",
        "\n",
        "for word in comment_tokens: # Go through every word in your tokens list\n",
        "    if (word not in stopwords_english and  # remove stopwords\n",
        "        word not in string.punctuation):  # remove punctuation\n",
        "        comment_clean.append(word)\n",
        "\n",
        "print('Removed stop words and punctuation:')\n",
        "print()\n",
        "print(comment_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAbXF8FbY_4T",
        "outputId": "c9397c7f-df73-4371-fbd7-2d1a6a4ef4b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m\n",
            "['go', 'ahead', 'and', 'try', ',', 'dude', '.', 'go', '-', 'fucking', '-', 'ahead', '.']\n",
            "\u001b[94m\n",
            "Removed stop words and punctuation:\n",
            "\n",
            "['go', 'ahead', 'try', 'dude', 'go', 'fucking', 'ahead']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary."
      ],
      "metadata": {
        "id": "3NUI0m1sZ31n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[92m')\n",
        "print(comment_clean)\n",
        "print('\\033[94m')\n",
        "\n",
        "# Instantiate stemming class\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Create an empty list to store the stems\n",
        "comment_stem = []\n",
        "\n",
        "for word in comment_clean:\n",
        "    stem_word = stemmer.stem(word)  # stemming word\n",
        "    comment_stem.append(stem_word)  # append to the list\n",
        "\n",
        "print('stemmed words:')\n",
        "print(comment_stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKu7AJVMZ56N",
        "outputId": "504c4d19-03c4-48b5-8456-bb1f88701b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m\n",
            "['go', 'ahead', 'try', 'dude', 'go', 'fucking', 'ahead']\n",
            "\u001b[94m\n",
            "stemmed words:\n",
            "['go', 'ahead', 'tri', 'dude', 'go', 'fuck', 'ahead']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# process_comment()\n",
        "Now we will create the process_comment() function that sums all the steps mentioned in the previous steps"
      ],
      "metadata": {
        "id": "VSr-_AQ-a23Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_comment(comment):\n",
        "\n",
        "  comment1 = comment.strip('\"')\n",
        "  comment1 = re.sub(r'https?://[^\\s\\n\\r]+', '', comment1)\n",
        "  comment1 = re.sub(r'#', '', comment1)\n",
        "  comment_tokens = tokenizer.tokenize(comment1)\n",
        "  comment_clean = []\n",
        "\n",
        "  for word in comment_tokens:\n",
        "      if (word not in stopwords_english and\n",
        "          word not in string.punctuation):\n",
        "          comment_clean.append(word)\n",
        "\n",
        "  comment_stem = []\n",
        "\n",
        "  for word in comment_clean:\n",
        "      stem_word = stemmer.stem(word)\n",
        "      comment_stem.append(stem_word)\n",
        "\n",
        "  return comment_stem\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qyl6Hs7Wa93f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our function\n"
      ],
      "metadata": {
        "id": "cxaClO-XdlSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a random test\n",
        "test_comment = positive_comments[3295]\n",
        "print(test_comment)\n",
        "print(process_comment(test_comment))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HklAWDlAdn7l",
        "outputId": "1b24b6f2-7caf-4481-8ba2-24d08f397dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"@ ede444 and that's all you've got to say is it well i won't be losing any sleep over it. Whether or not you are the same person or not you have the same stupid mentality. You're both like a couple of children so, easy to think you are one and the same. Your posts are the poorest in taste so you can't complain at mine. Calling me names just makes you look like a fool. If you think\\xa0I'm\\xa0showing off with any of my post it must mean you haven't done much with your own life. My posts are just a reflection of the truth to which you are obviously a green eyed monster. Nothing special to me or most people to say what languages one can speak or whether they've had the chance to meet important people.\\xa0Perhaps\\xa0you should concentrate on bettering your sad miserable life instead of posting childish comments to me.\"\n",
            "['ede', '444', \"that'\", 'got', 'say', 'well', 'lose', 'sleep', 'whether', 'person', 'stupid', 'mental', 'like', 'coupl', 'children', 'easi', 'think', 'one', 'post', 'poorest', 'tast', \"can't\", 'complain', 'mine', 'call', 'name', 'make', 'look', 'like', 'fool', 'think', 'xa0i', 'xa0show', 'post', 'must', 'mean', 'done', 'much', 'life', 'post', 'reflect', 'truth', 'obvious', 'green', 'eye', 'monster', 'noth', 'special', 'peopl', 'say', 'languag', 'one', 'speak', 'whether', \"they'v\", 'chanc', 'meet', 'import', 'peopl', 'xa0perhap', 'xa0you', 'concentr', 'better', 'sad', 'miser', 'life', 'instead', 'post', 'childish', 'comment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NitmF33ydqlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
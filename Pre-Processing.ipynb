{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXVxR-Xs-VdA"
      },
      "source": [
        "# **Project Title**: Detecting Insults in Social Commentary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyPeGBa2_P0f"
      },
      "source": [
        "## **Description:**\n",
        "In today's digital age, online discussions and social media have become an integral part of our lives. However, with the convenience of online communication comes the challenge of moderating and ensuring respectful discourse. This project aims to address a critical issue: **detecting and identifying insulting comments in social commentary**.\n",
        "\n",
        "The project focuses on the task of identifying comments that are intended to insult or demean participants in a conversation. These comments may contain profanity, offensive language, racial slurs, or other forms of disrespect. It's **important to note** that we are specifically interested in comments that target participants of the discussion, not public figures or celebrities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Vew3xK_k4D"
      },
      "source": [
        "## **Project Objectives**\n",
        "\n",
        "1. **Classifier Development:** The primary objective is to build a machine learning classifier that can accurately predict whether a given comment is insulting. This classifier should assign a probability score to each comment, indicating the likelihood of it being an insult.\n",
        "2. **Accuracy Priority:** Maximize accuracy while minimizing false positives and false negatives, achieving a balanced model.\n",
        "\n",
        "3. **Real-time Detection:** Develop a near-real-time model for automatic insult detection in online conversations.\n",
        "\n",
        "4. **Generalization:** Ensure the model handles diverse insults, including explicit and subtle forms, promoting generalizability.\n",
        "\n",
        "5. **Data Privacy:** Adhere to strict data protection standards, using comments solely for moderation purposes.\n",
        "\n",
        "6. **Scalable Solution:** Create a scalable system to process high volumes of user-generated content efficiently.\n",
        "\n",
        "7. **Ethical Compliance:** Address ethical concerns, including potential biases and fairness in the insult detection system.\n",
        "\n",
        "8. **Collaboration:** Collaborate with online platforms and communities for system implementation and refinement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (1.24.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (7.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (6.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas\n",
        "!pip3 install nltk\n",
        "!pip3 install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6-uZEo5Ad__"
      },
      "source": [
        "## **Data Understanding**\n",
        "\n",
        "To begin the project, let's analyze the available data. We'll create dataframes with the necessary input files, explore the data, and describe all the columns. Understanding the data is essential for developing an effective insult detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LrTcON5FCYNO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk                                # Python library for NLP\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Is9ZSfnMEMk5"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "verification_set = pd.read_csv('dataset/impermium_verification_set.csv')\n",
        "verification_labels = pd.read_csv('dataset/impermium_verification_labels.csv')\n",
        "test = pd.read_csv('dataset/test.csv')\n",
        "test_with_solutions = pd.read_csv('dataset/test_with_solutions.csv')\n",
        "train = pd.read_csv('dataset/train.csv')\n",
        "sample_submission_null = pd.read_csv('dataset/sample_submission_null.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Dataset Understanding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYhf5HZiNbBm"
      },
      "source": [
        "#### **Train (`train`)**:\n",
        "The training dataset used for training the machine learning model. It contains labeled data (comments with known insult labels) to develop and train the insult detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEghYg3YIscE",
        "outputId": "32c5d7c8-744f-4940-b270-7a8ceef96365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train:\n",
            "   Insult             Date                                            Comment\n",
            "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
            "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
            "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
            "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
            "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
            "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...\n",
            "6       0  20120503012628Z                      \"Yeah and where are you now?\"\n",
            "7       1              NaN  \"shut the fuck up. you and the rest of your fa...\n",
            "8       1  20120502173553Z  \"Either you are fake or extremely stupid...may...\n",
            "9       1  20120620160512Z  \"That you are an idiot who understands neither...\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the train data frame\n",
        "print(\"train:\")\n",
        "print(train.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yO6ydp_K8ld"
      },
      "source": [
        "\n",
        "\n",
        "#### **Verification Set (`verification_set`)**:\n",
        "This dataset is used for verification purposes during model development and testing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zpjg76DmEaVv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verification_set:\n",
            "   id  Insult             Date  \\\n",
            "0   1     NaN  20120603163526Z   \n",
            "1   2     NaN  20120531215447Z   \n",
            "2   3     NaN  20120823164228Z   \n",
            "3   4     NaN  20120826010752Z   \n",
            "4   5     NaN  20120602223825Z   \n",
            "5   6     NaN  20120603202442Z   \n",
            "6   7     NaN  20120603163604Z   \n",
            "7   8     NaN  20120602223902Z   \n",
            "8   9     NaN  20120528064125Z   \n",
            "9  10     NaN  20120603071243Z   \n",
            "\n",
            "                                             Comment        Usage  \n",
            "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
            "1              \"you're idiot.......................\"  PrivateTest  \n",
            "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
            "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
            "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n",
            "5  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...  PrivateTest  \n",
            "6  \"Oh go kiss the ass of a goat....and you DUMMY...  PrivateTest  \n",
            "7                  \"Not a chance Kid, you're wrong.\"  PrivateTest  \n",
            "8            \"On Some real Shit FUck LIVE JASMIN!!!\"  PrivateTest  \n",
            "9  \"ok but where the hell was it released?you all...  PrivateTest  \n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the verificationSet DataFrame\n",
        "print(\"verification_set:\")\n",
        "print(verification_set.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAoZXyrNZMr"
      },
      "source": [
        "#### **Verification Labels (`verification_labels`)**:\n",
        "This dataset provides labels for the 'verification_set,' with '0' indicating non-insulting and '1' indicating insulting comments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZaegVRPKE1yW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verification_labels:\n",
            "   id  Insult             Date  \\\n",
            "0   1       0  20120603163526Z   \n",
            "1   2       1  20120531215447Z   \n",
            "2   3       1  20120823164228Z   \n",
            "3   4       1  20120826010752Z   \n",
            "4   5       1  20120602223825Z   \n",
            "5   6       0  20120603202442Z   \n",
            "6   7       1  20120603163604Z   \n",
            "7   8       0  20120602223902Z   \n",
            "8   9       0  20120528064125Z   \n",
            "9  10       1  20120603071243Z   \n",
            "\n",
            "                                             Comment        Usage  \n",
            "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
            "1              \"you're idiot.......................\"  PrivateTest  \n",
            "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
            "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
            "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n",
            "5  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...  PrivateTest  \n",
            "6  \"Oh go kiss the ass of a goat....and you DUMMY...  PrivateTest  \n",
            "7                  \"Not a chance Kid, you're wrong.\"  PrivateTest  \n",
            "8            \"On Some real Shit FUck LIVE JASMIN!!!\"  PrivateTest  \n",
            "9  \"ok but where the hell was it released?you all...  PrivateTest  \n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the verification labels\n",
        "print(\"verification_labels:\")\n",
        "print(verification_labels.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJGpVAnANZzC"
      },
      "source": [
        "#### **Test (`test`)**:\n",
        "The main test dataset used to make predictions with the trained machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PHHvAn7cE8GP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test:\n",
            "   id             Date                                            Comment\n",
            "0   1  20120603163526Z                 \"like this if you are a tribe fan\"\n",
            "1   2  20120531215447Z              \"you're idiot.......................\"\n",
            "2   3  20120823164228Z  \"I am a woman Babs, and the only \"war on women...\n",
            "3   4  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
            "4   5  20120602223825Z  \"haha green me red you now loser whos winning ...\n",
            "5   6  20120603202442Z  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...\n",
            "6   7  20120603163604Z  \"Oh go kiss the ass of a goat....and you DUMMY...\n",
            "7   8  20120602223902Z                  \"Not a chance Kid, you're wrong.\"\n",
            "8   9  20120528064125Z            \"On Some real Shit FUck LIVE JASMIN!!!\"\n",
            "9  10  20120603071243Z  \"ok but where the hell was it released?you all...\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the test data frame\n",
        "print(\"test:\")\n",
        "print(test.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiDIyCJxOK6k"
      },
      "source": [
        "For the next step, we will focus solely on the 'train' dataset as we prepare to preprocess the data. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4xBInQXP_br"
      },
      "source": [
        "We extract positive and negative comments to two separate data frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "SmhLhOTfP8-_"
      },
      "outputs": [],
      "source": [
        "# Extract positive comments (Insult = 1) and select the 'Comment' column\n",
        "positive_comments = train[train['Insult'] == 1]['Comment']\n",
        "\n",
        "# Extract negative comments (Insult = 0) and select the 'Comment' column\n",
        "negative_comments = train[train['Insult'] == 0]['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgEmvjvrQKot",
        "outputId": "368f159c-b87c-4813-a22c-00684605dbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1049\n",
            "2898\n"
          ]
        }
      ],
      "source": [
        "print(len(positive_comments))\n",
        "print(len(negative_comments))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q__xgiESJ3m"
      },
      "source": [
        "Next, we will develop a text preprocessing function that will be applied to the comment text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ykCluZTukF"
      },
      "source": [
        "# Remove hyperlinks, Twitter marks and styles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_121PIewTxZg",
        "outputId": "ca6049da-ad7f-441c-d669-f8d90afdef44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "248     \"yeah I'm pathetic but your the idiot going ar...\n",
            "761     \"@peter8888,\\n\\nYou showed your true colors. I...\n",
            "835     \"Bitch, you replied to my comment, you stupid ...\n",
            "1009    \"Yo\\xa0@LukeEmery:disqus\\xa0you must be too du...\n",
            "1078    \"@berethor099 Go ahead and try, dude. Go - fuc...\n",
            "1145    \"@Pickle\\n\\nYou look like Brian Scalabrine's b...\n",
            "1428    \"@TeeBooWa\\n\\xa0HAH YOUR'E ONLY 14? FPFF AND Y...\n",
            "1585    \"Why don't you take your pathetic intimidation...\n",
            "1612               \"@justin_mia Perkins is a knucklehead\"\n",
            "1917    \"YOU ARE THE REAL @SSHO LE AND I HOPE PEOPLE O...\n",
            "2029    \"@Hoss \\xa0\\xa0@JenBroflovski\\xa0fuck you you ...\n",
            "2076               \"@caljb7 i'm on AAC then ya dumb fuck\"\n",
            "2105    \"@Enundr\\xa0lol.....You sir, are a jackass. wi...\n",
            "2456                   \"@FUCK:disqus\\xa0YOU CNN FAGGOTS.\"\n",
            "3079    \"@cnn-fcbba858f167b1594a66777bca:disqus \\n\\nYo...\n",
            "3100    \"@Bexxcc\\xa0\\xa0@bubzsucz\\n\\xa0YOU'RE THE SICK...\n",
            "3174    \"@lazerbyte Shut the fuck up -_- so where do y...\n",
            "3279    \"not funny jack@$$. you are a total db! friggi...\n",
            "3282    \"@RN506:disqus Read through the comments on th...\n",
            "3295    \"@ ede444 and that's all you've got to say is ...\n",
            "3306    \"Excellent command of the English language! On...\n",
            "3414         \"@bizora2:disqus you are a pathetic nobody.\"\n",
            "3677    \"@adamomars\\xa0\\xa0@Dieofnv\\xa0the very last t...\n",
            "3924    \"@Crissa:disqus LaRaza (The Race), NBP. Nation...\n",
            "Name: Comment, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Printing positive comments containing specific characters like '@' and 'https' to take as a sample\n",
        "print(positive_comments[positive_comments.str.contains('@|https', case=False)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlDMHcq5Vlrn",
        "outputId": "fc3eb515-1166-4894-c4eb-2448c8d986be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"@berethor099 Go ahead and try, dude. Go - fucking - ahead.\"\n"
          ]
        }
      ],
      "source": [
        "# We take a sample comment\n",
        "comment = positive_comments[1078]\n",
        "print(comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7BbW9crUCY1",
        "outputId": "3876c95a-953b-47ce-8896-9f3f03e7ffde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We download the stopwords from Natural Language Toolkit\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkjsqiYxUhl1",
        "outputId": "9e29ec2f-7618-4a1f-ed11-7b2f50347347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m\"@berethor099 Go ahead and try, dude. Go - fucking - ahead.\"\n",
            "\u001b[94m\n",
            "@berethor099 Go ahead and try, dude. Go - fucking - ahead.\n"
          ]
        }
      ],
      "source": [
        "print('\\033[92m' + comment)\n",
        "print('\\033[94m')\n",
        "\n",
        "# remove the \"\" from the end and start of the sentences\n",
        "comment1 = comment.strip('\"')\n",
        "\n",
        "# remove hyperlinks\n",
        "comment1 = re.sub(r'https?://[^\\s\\n\\r]+', '', comment1)\n",
        "\n",
        "# only removing the hash # sign from the word\n",
        "comment1 = re.sub(r'#', '', comment1)\n",
        "\n",
        "print(comment1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tTD6KiIYCXo"
      },
      "source": [
        "# Tokenize the string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtM17DnrYBSD",
        "outputId": "3907dccb-488f-499b-f428-59c409c23ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m@berethor099 Go ahead and try, dude. Go - fucking - ahead.\n",
            "\u001b[94m\n",
            "Tokenized string:\n",
            "['go', 'ahead', 'and', 'try', ',', 'dude', '.', 'go', '-', 'fucking', '-', 'ahead', '.']\n"
          ]
        }
      ],
      "source": [
        "print('\\033[92m' + comment1)\n",
        "print('\\033[94m')\n",
        "\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "comment_tokens = tokenizer.tokenize(comment1)\n",
        "\n",
        "print('Tokenized string:')\n",
        "print(comment_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuXmmGcSYma7"
      },
      "source": [
        "# Remove stop words and punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4B0K1XsYUEX",
        "outputId": "4672bd50-df31-4fe4-c183-dc859f961be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Due5IXoJZHUd"
      },
      "source": [
        "We might need to customize the stop words list for our applications. Since the model should diffrenciate between insults to users of the forum and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAbXF8FbY_4T",
        "outputId": "c9397c7f-df73-4371-fbd7-2d1a6a4ef4b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "['go', 'ahead', 'and', 'try', ',', 'dude', '.', 'go', '-', 'fucking', '-', 'ahead', '.']\n",
            "\u001b[94m\n",
            "Removed stop words and punctuation:\n",
            "\n",
            "['go', 'ahead', 'try', 'dude', 'go', 'fucking', 'ahead']\n"
          ]
        }
      ],
      "source": [
        "print('\\033[92m')\n",
        "print(comment_tokens)\n",
        "print('\\033[94m')\n",
        "comment_clean = []\n",
        "\n",
        "for word in comment_tokens: # Go through every word in your tokens list\n",
        "    if (word not in stopwords_english and  # remove stopwords\n",
        "        word not in string.punctuation):  # remove punctuation\n",
        "        comment_clean.append(word)\n",
        "\n",
        "print('Removed stop words and punctuation:')\n",
        "print()\n",
        "print(comment_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NUI0m1sZ31n"
      },
      "source": [
        "# Stemming\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKu7AJVMZ56N",
        "outputId": "504c4d19-03c4-48b5-8456-bb1f88701b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "['go', 'ahead', 'try', 'dude', 'go', 'fucking', 'ahead']\n",
            "\u001b[94m\n",
            "stemmed words:\n",
            "['go', 'ahead', 'tri', 'dude', 'go', 'fuck', 'ahead']\n"
          ]
        }
      ],
      "source": [
        "print('\\033[92m')\n",
        "print(comment_clean)\n",
        "print('\\033[94m')\n",
        "\n",
        "# Instantiate stemming class\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Create an empty list to store the stems\n",
        "comment_stem = []\n",
        "\n",
        "for word in comment_clean:\n",
        "    stem_word = stemmer.stem(word)  # stemming word\n",
        "    comment_stem.append(stem_word)  # append to the list\n",
        "\n",
        "print('stemmed words:')\n",
        "print(comment_stem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remove Duplicate Words\n",
        "Our next step is to remove duplicate words from the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NitmF33ydqlc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "['go', 'ahead', 'tri', 'dude', 'go', 'fuck', 'ahead']\n",
            "\u001b[94m\n",
            "['go', 'ahead', 'tri', 'dude', 'fuck']\n"
          ]
        }
      ],
      "source": [
        "print('\\033[92m')\n",
        "print(comment_stem)\n",
        "print('\\033[94m')\n",
        "#create an empty set to store unique words \n",
        "unique_words = set()\n",
        "# Create an empty list to store the stems\n",
        "comment_NoDup = []\n",
        "\n",
        "# iterate through the stemmed words \n",
        "for word in comment_stem:\n",
        " if word not in unique_words:\n",
        "    unique_words.add(word)\n",
        "    comment_NoDup.append(word)\n",
        "   \n",
        "print(comment_NoDup) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSr-_AQ-a23Y"
      },
      "source": [
        "# process_comment()\n",
        "Now we will create the process_comment() function that sums all the steps mentioned in the previous steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qyl6Hs7Wa93f"
      },
      "outputs": [],
      "source": [
        "def process_comment(comment):\n",
        "\n",
        "  comment1 = comment.strip('\"')\n",
        "  comment1 = re.sub(r'https?://[^\\s\\n\\r]+', '', comment1)\n",
        "  comment1 = re.sub(r'#', '', comment1)\n",
        "  comment_tokens = tokenizer.tokenize(comment1)\n",
        "  comment_clean = []\n",
        "  unique_words = set()\n",
        "  comment_NoDup = []\n",
        "  for word in comment_tokens:\n",
        "      if (word not in stopwords_english and\n",
        "          word not in string.punctuation):\n",
        "          comment_clean.append(word)\n",
        "\n",
        "  comment_stem = []\n",
        "\n",
        "  for word in comment_clean:\n",
        "      stem_word = stemmer.stem(word)\n",
        "      comment_stem.append(stem_word)\n",
        "\n",
        "  for word in comment_stem:\n",
        "    if word not in unique_words:\n",
        "       unique_words.add(word)\n",
        "       comment_NoDup.append(word)\n",
        "    return comment_NoDup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaClO-XdlSE"
      },
      "source": [
        "Let's test our function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HklAWDlAdn7l",
        "outputId": "1b24b6f2-7caf-4481-8ba2-24d08f397dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m \"@ ede444 and that's all you've got to say is it well i won't be losing any sleep over it. Whether or not you are the same person or not you have the same stupid mentality. You're both like a couple of children so, easy to think you are one and the same. Your posts are the poorest in taste so you can't complain at mine. Calling me names just makes you look like a fool. If you think\\xa0I'm\\xa0showing off with any of my post it must mean you haven't done much with your own life. My posts are just a reflection of the truth to which you are obviously a green eyed monster. Nothing special to me or most people to say what languages one can speak or whether they've had the chance to meet important people.\\xa0Perhaps\\xa0you should concentrate on bettering your sad miserable life instead of posting childish comments to me.\"\n",
            "\u001b[94m\n",
            "['ede']\n",
            "['go', 'ahead', 'tri', 'dude', 'fuck']\n"
          ]
        }
      ],
      "source": [
        "# Pick a random test\n",
        "test_comment = positive_comments[3295]\n",
        "print('\\033[92m', test_comment)\n",
        "print('\\033[94m')\n",
        "\n",
        "print(process_comment(test_comment))\n",
        "print(comment_NoDup)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

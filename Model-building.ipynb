{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXVxR-Xs-VdA"
      },
      "source": [
        "# **Project Title**: Detecting Insults in Social Commentary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyPeGBa2_P0f"
      },
      "source": [
        "## **Business Understanding:**\n",
        "### **Description**\n",
        "In today's digital age, online discussions and social media have become an integral part of our lives. However, with the convenience of online communication comes the challenge of moderating and ensuring respectful discourse. This project aims to address a critical issue: **detecting and identifying insulting comments in social commentary**.\n",
        "\n",
        "The project focuses on the task of identifying comments that are intended to insult or demean participants in a conversation. These comments may contain profanity, offensive language, racial slurs, or other forms of disrespect. It's **important to note** that we are specifically interested in comments that target participants of the discussion, not public figures or celebrities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Vew3xK_k4D"
      },
      "source": [
        "## **Project Objectives**\n",
        "### **Main-Goals**\n",
        " **Classifier Development:** The primary objective is to build a machine learning classifier that can accurately predict whether a given comment is insulting. This classifier should assign a probability score to each comment, indicating the likelihood of it being an insult.\n",
        "### **Sub-Goals**\n",
        "- **Accuracy Priority:** Maximize accuracy while minimizing false positives and false negatives, achieving a balanced model.\n",
        "  \n",
        "- **Generalization:** Ensure the model handles diverse insults, including explicit and subtle forms, promoting generalizability.\n",
        "\n",
        "- **Data Privacy:** Adhere to strict data protection standards, using comments solely for moderation purposes.\n",
        "\n",
        "- **Scalable Solution:** Create a scalable system to process high volumes of user-generated content efficiently.\n",
        "\n",
        "- **Ethical Compliance:** Address ethical concerns, including potential biases and fairness in the insult detection system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (1.24.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (7.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (6.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas\n",
        "!pip3 install nltk\n",
        "!pip3 install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6-uZEo5Ad__"
      },
      "source": [
        "# **Data Understanding**\n",
        "\n",
        "To begin the project, let's analyze the available data. We'll create dataframes with the necessary input files, explore the data, and describe all the columns. Understanding the data is essential for developing an effective insult detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LrTcON5FCYNO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk                                # Python library for NLP\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import word_tokenize    # module for tokenizing strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Is9ZSfnMEMk5"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "verification_set = pd.read_csv('dataset/impermium_verification_set.csv')\n",
        "verification_labels = pd.read_csv('dataset/impermium_verification_labels.csv')\n",
        "test = pd.read_csv('dataset/test.csv')\n",
        "test_with_solutions = pd.read_csv('dataset/test_with_solutions.csv')\n",
        "train = pd.read_csv('dataset/train.csv')\n",
        "sample_submission_null = pd.read_csv('dataset/sample_submission_null.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Dataset Understanding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYhf5HZiNbBm"
      },
      "source": [
        "#### **Train (`train`)**:\n",
        "The training dataset used for training the machine learning model. It contains labeled data (comments with known insult labels) to develop and train the insult detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEghYg3YIscE",
        "outputId": "32c5d7c8-744f-4940-b270-7a8ceef96365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train:\n",
            "   Insult             Date                                            Comment\n",
            "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
            "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
            "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
            "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
            "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
            "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...\n",
            "6       0  20120503012628Z                      \"Yeah and where are you now?\"\n",
            "7       1              NaN  \"shut the fuck up. you and the rest of your fa...\n",
            "8       1  20120502173553Z  \"Either you are fake or extremely stupid...may...\n",
            "9       1  20120620160512Z  \"That you are an idiot who understands neither...\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the train data frame\n",
        "print(\"train:\")\n",
        "print(train.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yO6ydp_K8ld"
      },
      "source": [
        "\n",
        "\n",
        "#### **Verification Set (`verification_set`)**:\n",
        "This dataset is used for verification purposes during model development and testing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zpjg76DmEaVv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verification_set:\n",
            "   id  Insult             Date  \\\n",
            "0   1     NaN  20120603163526Z   \n",
            "1   2     NaN  20120531215447Z   \n",
            "2   3     NaN  20120823164228Z   \n",
            "3   4     NaN  20120826010752Z   \n",
            "4   5     NaN  20120602223825Z   \n",
            "5   6     NaN  20120603202442Z   \n",
            "6   7     NaN  20120603163604Z   \n",
            "7   8     NaN  20120602223902Z   \n",
            "8   9     NaN  20120528064125Z   \n",
            "9  10     NaN  20120603071243Z   \n",
            "\n",
            "                                             Comment        Usage  \n",
            "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
            "1              \"you're idiot.......................\"  PrivateTest  \n",
            "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
            "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
            "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n",
            "5  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...  PrivateTest  \n",
            "6  \"Oh go kiss the ass of a goat....and you DUMMY...  PrivateTest  \n",
            "7                  \"Not a chance Kid, you're wrong.\"  PrivateTest  \n",
            "8            \"On Some real Shit FUck LIVE JASMIN!!!\"  PrivateTest  \n",
            "9  \"ok but where the hell was it released?you all...  PrivateTest  \n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the verificationSet DataFrame\n",
        "print(\"verification_set:\")\n",
        "print(verification_set.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAoZXyrNZMr"
      },
      "source": [
        "#### **Verification Labels (`verification_labels`)**:\n",
        "This dataset provides labels for the 'verification_set,' with '0' indicating non-insulting and '1' indicating insulting comments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZaegVRPKE1yW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verification_labels:\n",
            "   id  Insult             Date  \\\n",
            "0   1       0  20120603163526Z   \n",
            "1   2       1  20120531215447Z   \n",
            "2   3       1  20120823164228Z   \n",
            "3   4       1  20120826010752Z   \n",
            "4   5       1  20120602223825Z   \n",
            "5   6       0  20120603202442Z   \n",
            "6   7       1  20120603163604Z   \n",
            "7   8       0  20120602223902Z   \n",
            "8   9       0  20120528064125Z   \n",
            "9  10       1  20120603071243Z   \n",
            "\n",
            "                                             Comment        Usage  \n",
            "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
            "1              \"you're idiot.......................\"  PrivateTest  \n",
            "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
            "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
            "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n",
            "5  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...  PrivateTest  \n",
            "6  \"Oh go kiss the ass of a goat....and you DUMMY...  PrivateTest  \n",
            "7                  \"Not a chance Kid, you're wrong.\"  PrivateTest  \n",
            "8            \"On Some real Shit FUck LIVE JASMIN!!!\"  PrivateTest  \n",
            "9  \"ok but where the hell was it released?you all...  PrivateTest  \n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the verification labels\n",
        "print(\"verification_labels:\")\n",
        "print(verification_labels.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJGpVAnANZzC"
      },
      "source": [
        "#### **Test (`test`)**:\n",
        "The main test dataset used to make predictions with the trained machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PHHvAn7cE8GP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test:\n",
            "   id             Date                                            Comment\n",
            "0   1  20120603163526Z                 \"like this if you are a tribe fan\"\n",
            "1   2  20120531215447Z              \"you're idiot.......................\"\n",
            "2   3  20120823164228Z  \"I am a woman Babs, and the only \"war on women...\n",
            "3   4  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
            "4   5  20120602223825Z  \"haha green me red you now loser whos winning ...\n",
            "5   6  20120603202442Z  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...\n",
            "6   7  20120603163604Z  \"Oh go kiss the ass of a goat....and you DUMMY...\n",
            "7   8  20120602223902Z                  \"Not a chance Kid, you're wrong.\"\n",
            "8   9  20120528064125Z            \"On Some real Shit FUck LIVE JASMIN!!!\"\n",
            "9  10  20120603071243Z  \"ok but where the hell was it released?you all...\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the test data frame\n",
        "print(\"test:\")\n",
        "print(test.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiDIyCJxOK6k"
      },
      "source": [
        "For the next step, we will focus solely on the 'train' dataset as we prepare to preprocess the data. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4xBInQXP_br"
      },
      "source": [
        "We extract positive and negative comments to two separate data frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SmhLhOTfP8-_"
      },
      "outputs": [],
      "source": [
        "# Extract positive comments (Insult = 1) and select the 'Comment' column\n",
        "positive_comments = train[train['Insult'] == 1]['Comment']\n",
        "\n",
        "# Extract negative comments (Insult = 0) and select the 'Comment' column\n",
        "negative_comments = train[train['Insult'] == 0]['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgEmvjvrQKot",
        "outputId": "368f159c-b87c-4813-a22c-00684605dbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1049\n",
            "2898\n"
          ]
        }
      ],
      "source": [
        "print(len(positive_comments))\n",
        "print(len(negative_comments))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q__xgiESJ3m"
      },
      "source": [
        "Next, we will develop a text preprocessing function that will be applied to the comment text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Data Cleaning:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ykCluZTukF"
      },
      "source": [
        "##### **-Remove hyperlinks, Twitter marks and styles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_121PIewTxZg",
        "outputId": "ca6049da-ad7f-441c-d669-f8d90afdef44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "248     \"yeah I'm pathetic but your the idiot going ar...\n",
            "761     \"@peter8888,\\n\\nYou showed your true colors. I...\n",
            "835     \"Bitch, you replied to my comment, you stupid ...\n",
            "1009    \"Yo\\xa0@LukeEmery:disqus\\xa0you must be too du...\n",
            "1078    \"@berethor099 Go ahead and try, dude. Go - fuc...\n",
            "1145    \"@Pickle\\n\\nYou look like Brian Scalabrine's b...\n",
            "1428    \"@TeeBooWa\\n\\xa0HAH YOUR'E ONLY 14? FPFF AND Y...\n",
            "1585    \"Why don't you take your pathetic intimidation...\n",
            "1612               \"@justin_mia Perkins is a knucklehead\"\n",
            "1917    \"YOU ARE THE REAL @SSHO LE AND I HOPE PEOPLE O...\n",
            "2029    \"@Hoss \\xa0\\xa0@JenBroflovski\\xa0fuck you you ...\n",
            "2076               \"@caljb7 i'm on AAC then ya dumb fuck\"\n",
            "2105    \"@Enundr\\xa0lol.....You sir, are a jackass. wi...\n",
            "2456                   \"@FUCK:disqus\\xa0YOU CNN FAGGOTS.\"\n",
            "3079    \"@cnn-fcbba858f167b1594a66777bca:disqus \\n\\nYo...\n",
            "3100    \"@Bexxcc\\xa0\\xa0@bubzsucz\\n\\xa0YOU'RE THE SICK...\n",
            "3174    \"@lazerbyte Shut the fuck up -_- so where do y...\n",
            "3279    \"not funny jack@$$. you are a total db! friggi...\n",
            "3282    \"@RN506:disqus Read through the comments on th...\n",
            "3295    \"@ ede444 and that's all you've got to say is ...\n",
            "3306    \"Excellent command of the English language! On...\n",
            "3414         \"@bizora2:disqus you are a pathetic nobody.\"\n",
            "3677    \"@adamomars\\xa0\\xa0@Dieofnv\\xa0the very last t...\n",
            "3924    \"@Crissa:disqus LaRaza (The Race), NBP. Nation...\n",
            "Name: Comment, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Printing positive comments containing specific characters like '@' and 'https' to take as a sample\n",
        "print(positive_comments[positive_comments.str.contains('@|https', case=False)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlDMHcq5Vlrn",
        "outputId": "fc3eb515-1166-4894-c4eb-2448c8d986be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"@berethor099 Go ahead and try, dude. Go - fucking - ahead.\"\n"
          ]
        }
      ],
      "source": [
        "# We take a sample comment\n",
        "comment = positive_comments[1078]\n",
        "print(comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7BbW9crUCY1",
        "outputId": "3876c95a-953b-47ce-8896-9f3f03e7ffde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We download the stopwords from Natural Language Toolkit\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkjsqiYxUhl1",
        "outputId": "9e29ec2f-7618-4a1f-ed11-7b2f50347347"
      },
      "outputs": [],
      "source": [
        "# Function to remove double quotation marks from the beginning and end of the comment\n",
        "def strip_quotes(comment):\n",
        "    \"\"\"Remove quotes from the beginning and end of the comment.\"\"\"\n",
        "    return comment.strip('\"')\n",
        "\n",
        "# Function to remove hyperlinks (URLs) from the comment\n",
        "def remove_hyperlinks(comment):\n",
        "    \"\"\"Remove hyperlinks from the comment.\"\"\"\n",
        "    # Use regular expression to match and remove hyperlinks\n",
        "    return re.sub(r'https?://[^\\s\\n\\r]+', '', comment)\n",
        "\n",
        "# Function to remove hash symbols (#) from the comment\n",
        "def remove_hash_symbols(comment):\n",
        "    \"\"\"Remove hash # symbols from the comment.\"\"\"\n",
        "    # Use regular expression to match and remove hash symbols\n",
        "    return re.sub(r'#', '', comment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tTD6KiIYCXo"
      },
      "source": [
        "##### **-Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtM17DnrYBSD",
        "outputId": "3907dccb-488f-499b-f428-59c409c23ad9"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize the comment into words\n",
        "def tokenize_comment(comment):\n",
        "    \"\"\"Tokenize the comment into words.\"\"\"\n",
        "    # Use NLTK's word_tokenize to split the comment into individual words (tokens)\n",
        "    return word_tokenize(comment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuXmmGcSYma7"
      },
      "source": [
        "#### **-Remove stop words and punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4B0K1XsYUEX",
        "outputId": "4672bd50-df31-4fe4-c183-dc859f961be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Due5IXoJZHUd"
      },
      "source": [
        "We might need to customize the stop words list for our applications. Since the model should diffrenciate between insults to users of the forum and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAbXF8FbY_4T",
        "outputId": "c9397c7f-df73-4371-fbd7-2d1a6a4ef4b6"
      },
      "outputs": [],
      "source": [
        "# Function to remove stopwords and punctuation from the comment\n",
        "def clean_comment(comment_tokens, stopwords_english):\n",
        "    \"\"\"\n",
        "    Remove stopwords and punctuation from the comment.\n",
        "\n",
        "    Args:\n",
        "        comment_tokens (list): A list of tokens (words) from the comment.\n",
        "        stopwords_english (set): A set of English stopwords to be removed.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of cleaned tokens with stopwords and punctuation removed.\n",
        "    \"\"\"\n",
        "    comment_clean = []\n",
        "\n",
        "    # Iterate through each word in the comment_tokens list\n",
        "    for word in comment_tokens:\n",
        "        # Check if the word is not in stopwords_english and not in string.punctuation\n",
        "        if (word not in stopwords_english and\n",
        "            word not in string.punctuation):\n",
        "            comment_clean.append(word)\n",
        "\n",
        "    return comment_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NUI0m1sZ31n"
      },
      "source": [
        "##### **-Stemming**\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKu7AJVMZ56N",
        "outputId": "504c4d19-03c4-48b5-8456-bb1f88701b04"
      },
      "outputs": [],
      "source": [
        "# Function to stem (reduce to their root form) the words in the comment\n",
        "def stem_comment(comment_clean):\n",
        "    \"\"\"\n",
        "    Stem the words in the comment.\n",
        "\n",
        "    Args:\n",
        "        comment_clean (list): A list of cleaned tokens (words) from the comment.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of stemmed tokens (words).\n",
        "    \"\"\"\n",
        "    # Create a PorterStemmer object\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Apply stemming to each word in the comment_clean list\n",
        "    comment_stem = [stemmer.stem(word) for word in comment_clean]\n",
        "\n",
        "    return comment_stem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **-Remove Duplicate Words**\n",
        "Our next step is to remove duplicate words from the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NitmF33ydqlc"
      },
      "outputs": [],
      "source": [
        "# Function to remove duplicate words from the comment and return a list of unique words\n",
        "def remove_duplicates(comment_stem):\n",
        "    \"\"\"\n",
        "    Remove duplicate words from the comment and return a list of unique words.\n",
        "\n",
        "    Args:\n",
        "        comment_stem (list): A list of stemmed tokens (words) from the comment.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of unique words from the input comment_stem list.\n",
        "    \"\"\"\n",
        "    unique_words = set()  # Create an empty set to store unique words\n",
        "    comment_NoDup = []    # Create an empty list to store unique words in order\n",
        "\n",
        "    # Iterate through each word in the comment_stem list\n",
        "    for word in comment_stem:\n",
        "        # Check if the word is not already in the set of unique_words\n",
        "        if word not in unique_words:\n",
        "            # If not in the set, add it to the set and the comment_NoDup list\n",
        "            unique_words.add(word)\n",
        "            comment_NoDup.append(word)\n",
        "\n",
        "    return comment_NoDup  # Return the list of unique words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSr-_AQ-a23Y"
      },
      "source": [
        "#### **-process_comment()**\n",
        "Now we will create the process_comment() function that sums all the steps mentioned in the previous steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qyl6Hs7Wa93f"
      },
      "outputs": [],
      "source": [
        "# Function to process the comment through a series of functions and return the result\n",
        "def process_comment(comment, stopwords_english):\n",
        "    \"\"\"\n",
        "    Process the comment through a series of functions and return the result.\n",
        "\n",
        "    Args:\n",
        "        comment (str): The input comment to be processed.\n",
        "        stopwords_english (set): A set of English stopwords to be used in cleaning.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of unique, stemmed, and cleaned words from the comment.\n",
        "    \"\"\"\n",
        "    # Step 1: Remove double quotation marks from the comment\n",
        "    comment1 = strip_quotes(comment)\n",
        "\n",
        "    # Step 2: Remove hyperlinks from the comment\n",
        "    comment2 = remove_hyperlinks(comment1)\n",
        "\n",
        "    # Step 3: Remove hash # symbols from the comment\n",
        "    comment3 = remove_hash_symbols(comment2)\n",
        "\n",
        "    # Step 4: Tokenize the cleaned comment into words\n",
        "    comment_tokens = tokenize_comment(comment3)\n",
        "\n",
        "    # Step 5: Remove stopwords and punctuation from the tokenized comment\n",
        "    comment_clean = clean_comment(comment_tokens, stopwords_english)\n",
        "\n",
        "    # Step 6: Stem the cleaned words in the comment\n",
        "    comment_stem = stem_comment(comment_clean)\n",
        "\n",
        "    # Step 7: Remove duplicate words and return a list of unique words\n",
        "    comment_NoDup = remove_duplicates(comment_stem)\n",
        "\n",
        "    # Return the final result, which is a list of unique, stemmed, and cleaned words\n",
        "    return comment_NoDup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaClO-XdlSE"
      },
      "source": [
        "Let's test our function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HklAWDlAdn7l",
        "outputId": "1b24b6f2-7caf-4481-8ba2-24d08f397dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m \"@ ede444 and that's all you've got to say is it well i won't be losing any sleep over it. Whether or not you are the same person or not you have the same stupid mentality. You're both like a couple of children so, easy to think you are one and the same. Your posts are the poorest in taste so you can't complain at mine. Calling me names just makes you look like a fool. If you think\\xa0I'm\\xa0showing off with any of my post it must mean you haven't done much with your own life. My posts are just a reflection of the truth to which you are obviously a green eyed monster. Nothing special to me or most people to say what languages one can speak or whether they've had the chance to meet important people.\\xa0Perhaps\\xa0you should concentrate on bettering your sad miserable life instead of posting childish comments to me.\"\n",
            "\u001b[94m\n",
            "['ede444', \"'s\", \"'ve\", 'got', 'say', 'well', 'wo', \"n't\", 'lose', 'sleep', 'whether', 'person', 'stupid', 'mental', 'you', \"'re\", 'like', 'coupl', 'children', 'easi', 'think', 'one', 'your', 'post', 'poorest', 'tast', 'ca', 'complain', 'mine', 'call', 'name', 'make', 'look', 'fool', 'if', \"think\\\\xa0i'm\\\\xa0show\", 'must', 'mean', 'done', 'much', 'life', 'my', 'reflect', 'truth', 'obvious', 'green', 'eye', 'monster', 'noth', 'special', 'peopl', 'languag', 'speak', 'chanc', 'meet', 'import', 'people.\\\\xa0perhaps\\\\xa0y', 'concentr', 'better', 'sad', 'miser', 'instead', 'childish', 'comment']\n"
          ]
        }
      ],
      "source": [
        "# Pick a random test comment from a list of positive comments\n",
        "test_comment = positive_comments[3295]\n",
        "\n",
        "# Print the test comment in green color for visibility\n",
        "print('\\033[92m', test_comment)\n",
        "\n",
        "# Print a line break with blue color for separation\n",
        "print('\\033[94m')\n",
        "\n",
        "# Process the test comment using the process_comment function\n",
        "processed_result = process_comment(test_comment, stopwords_english)\n",
        "\n",
        "# Print the processed result, which is a list of unique, stemmed, and cleaned words\n",
        "print(processed_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

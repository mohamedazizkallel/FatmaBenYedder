{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXVxR-Xs-VdA"
      },
      "source": [
        "# **Project Title**: Detecting Insults in Social Commentary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyPeGBa2_P0f"
      },
      "source": [
        "## **Description:**\n",
        "In today's digital age, online discussions and social media have become an integral part of our lives. However, with the convenience of online communication comes the challenge of moderating and ensuring respectful discourse. This project aims to address a critical issue: **detecting and identifying insulting comments in social commentary**.\n",
        "\n",
        "The project focuses on the task of identifying comments that are intended to insult or demean participants in a conversation. These comments may contain profanity, offensive language, racial slurs, or other forms of disrespect. It's **important to note** that we are specifically interested in comments that target participants of the discussion, not public figures or celebrities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Vew3xK_k4D"
      },
      "source": [
        "## **Project Objectives**\n",
        "### **Main-Goals**\n",
        " **Classifier Development:** The primary objective is to build a machine learning classifier that can accurately predict whether a given comment is insulting. This classifier should assign a probability score to each comment, indicating the likelihood of it being an insult.\n",
        "### **Sub-Goals**\n",
        "- **Accuracy Priority:** Maximize accuracy while minimizing false positives and false negatives, achieving a balanced model.\n",
        "  \n",
        "- **Generalization:** Ensure the model handles diverse insults, including explicit and subtle forms, promoting generalizability.\n",
        "\n",
        "- **Data Privacy:** Adhere to strict data protection standards, using comments solely for moderation purposes.\n",
        "\n",
        "- **Scalable Solution:** Create a scalable system to process high volumes of user-generated content efficiently.\n",
        "\n",
        "- **Ethical Compliance:** Address ethical concerns, including potential biases and fairness in the insult detection system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.3)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (1.24.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.7.3)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (7.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from matplotlib) (6.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas\n",
        "!pip3 install nltk\n",
        "!pip3 install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6-uZEo5Ad__"
      },
      "source": [
        "## **Data Understanding**\n",
        "\n",
        "To begin the project, let's analyze the available data. We'll create dataframes with the necessary input files, explore the data, and describe all the columns. Understanding the data is essential for developing an effective insult detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LrTcON5FCYNO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk                                # Python library for NLP\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import word_tokenize    # module for tokenizing strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Is9ZSfnMEMk5"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "verification_set = pd.read_csv('dataset/impermium_verification_set.csv')\n",
        "verification_labels = pd.read_csv('dataset/impermium_verification_labels.csv')\n",
        "test = pd.read_csv('dataset/test.csv')\n",
        "test_with_solutions = pd.read_csv('dataset/test_with_solutions.csv')\n",
        "train = pd.read_csv('dataset/train.csv')\n",
        "sample_submission_null = pd.read_csv('dataset/sample_submission_null.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Dataset Understanding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYhf5HZiNbBm"
      },
      "source": [
        "#### **Train (`train`)**:\n",
        "The training dataset used for training the machine learning model. It contains labeled data (comments with known insult labels) to develop and train the insult detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEghYg3YIscE",
        "outputId": "32c5d7c8-744f-4940-b270-7a8ceef96365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train:\n",
            "   Insult             Date                                            Comment\n",
            "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
            "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
            "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
            "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
            "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
            "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...\n",
            "6       0  20120503012628Z                      \"Yeah and where are you now?\"\n",
            "7       1              NaN  \"shut the fuck up. you and the rest of your fa...\n",
            "8       1  20120502173553Z  \"Either you are fake or extremely stupid...may...\n",
            "9       1  20120620160512Z  \"That you are an idiot who understands neither...\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the train data frame\n",
        "print(\"train:\")\n",
        "print(train.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yO6ydp_K8ld"
      },
      "source": [
        "\n",
        "\n",
        "#### **Verification Set (`verification_set`)**:\n",
        "This dataset is used for verification purposes during model development and testing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zpjg76DmEaVv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verification_set:\n",
            "   id  Insult             Date  \\\n",
            "0   1     NaN  20120603163526Z   \n",
            "1   2     NaN  20120531215447Z   \n",
            "2   3     NaN  20120823164228Z   \n",
            "3   4     NaN  20120826010752Z   \n",
            "4   5     NaN  20120602223825Z   \n",
            "5   6     NaN  20120603202442Z   \n",
            "6   7     NaN  20120603163604Z   \n",
            "7   8     NaN  20120602223902Z   \n",
            "8   9     NaN  20120528064125Z   \n",
            "9  10     NaN  20120603071243Z   \n",
            "\n",
            "                                             Comment        Usage  \n",
            "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
            "1              \"you're idiot.......................\"  PrivateTest  \n",
            "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
            "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
            "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n",
            "5  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...  PrivateTest  \n",
            "6  \"Oh go kiss the ass of a goat....and you DUMMY...  PrivateTest  \n",
            "7                  \"Not a chance Kid, you're wrong.\"  PrivateTest  \n",
            "8            \"On Some real Shit FUck LIVE JASMIN!!!\"  PrivateTest  \n",
            "9  \"ok but where the hell was it released?you all...  PrivateTest  \n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the verificationSet DataFrame\n",
        "print(\"verification_set:\")\n",
        "print(verification_set.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAoZXyrNZMr"
      },
      "source": [
        "#### **Verification Labels (`verification_labels`)**:\n",
        "This dataset provides labels for the 'verification_set,' with '0' indicating non-insulting and '1' indicating insulting comments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZaegVRPKE1yW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verification_labels:\n",
            "   id  Insult             Date  \\\n",
            "0   1       0  20120603163526Z   \n",
            "1   2       1  20120531215447Z   \n",
            "2   3       1  20120823164228Z   \n",
            "3   4       1  20120826010752Z   \n",
            "4   5       1  20120602223825Z   \n",
            "5   6       0  20120603202442Z   \n",
            "6   7       1  20120603163604Z   \n",
            "7   8       0  20120602223902Z   \n",
            "8   9       0  20120528064125Z   \n",
            "9  10       1  20120603071243Z   \n",
            "\n",
            "                                             Comment        Usage  \n",
            "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
            "1              \"you're idiot.......................\"  PrivateTest  \n",
            "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
            "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
            "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n",
            "5  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...  PrivateTest  \n",
            "6  \"Oh go kiss the ass of a goat....and you DUMMY...  PrivateTest  \n",
            "7                  \"Not a chance Kid, you're wrong.\"  PrivateTest  \n",
            "8            \"On Some real Shit FUck LIVE JASMIN!!!\"  PrivateTest  \n",
            "9  \"ok but where the hell was it released?you all...  PrivateTest  \n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the verification labels\n",
        "print(\"verification_labels:\")\n",
        "print(verification_labels.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJGpVAnANZzC"
      },
      "source": [
        "#### **Test (`test`)**:\n",
        "The main test dataset used to make predictions with the trained machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PHHvAn7cE8GP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test:\n",
            "   id             Date                                            Comment\n",
            "0   1  20120603163526Z                 \"like this if you are a tribe fan\"\n",
            "1   2  20120531215447Z              \"you're idiot.......................\"\n",
            "2   3  20120823164228Z  \"I am a woman Babs, and the only \"war on women...\n",
            "3   4  20120826010752Z  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
            "4   5  20120602223825Z  \"haha green me red you now loser whos winning ...\n",
            "5   6  20120603202442Z  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...\n",
            "6   7  20120603163604Z  \"Oh go kiss the ass of a goat....and you DUMMY...\n",
            "7   8  20120602223902Z                  \"Not a chance Kid, you're wrong.\"\n",
            "8   9  20120528064125Z            \"On Some real Shit FUck LIVE JASMIN!!!\"\n",
            "9  10  20120603071243Z  \"ok but where the hell was it released?you all...\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 rows of the test data frame\n",
        "print(\"test:\")\n",
        "print(test.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiDIyCJxOK6k"
      },
      "source": [
        "For the next step, we will focus solely on the 'train' dataset as we prepare to preprocess the data. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4xBInQXP_br"
      },
      "source": [
        "We extract positive and negative comments to two separate data frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SmhLhOTfP8-_"
      },
      "outputs": [],
      "source": [
        "# Extract positive comments (Insult = 1) and select the 'Comment' column\n",
        "positive_comments = train[train['Insult'] == 1]['Comment']\n",
        "\n",
        "# Extract negative comments (Insult = 0) and select the 'Comment' column\n",
        "negative_comments = train[train['Insult'] == 0]['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgEmvjvrQKot",
        "outputId": "368f159c-b87c-4813-a22c-00684605dbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1049\n",
            "2898\n"
          ]
        }
      ],
      "source": [
        "print(len(positive_comments))\n",
        "print(len(negative_comments))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q__xgiESJ3m"
      },
      "source": [
        "Next, we will develop a text preprocessing function that will be applied to the comment text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ykCluZTukF"
      },
      "source": [
        "# Remove hyperlinks, Twitter marks and styles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_121PIewTxZg",
        "outputId": "ca6049da-ad7f-441c-d669-f8d90afdef44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "248     \"yeah I'm pathetic but your the idiot going ar...\n",
            "761     \"@peter8888,\\n\\nYou showed your true colors. I...\n",
            "835     \"Bitch, you replied to my comment, you stupid ...\n",
            "1009    \"Yo\\xa0@LukeEmery:disqus\\xa0you must be too du...\n",
            "1078    \"@berethor099 Go ahead and try, dude. Go - fuc...\n",
            "1145    \"@Pickle\\n\\nYou look like Brian Scalabrine's b...\n",
            "1428    \"@TeeBooWa\\n\\xa0HAH YOUR'E ONLY 14? FPFF AND Y...\n",
            "1585    \"Why don't you take your pathetic intimidation...\n",
            "1612               \"@justin_mia Perkins is a knucklehead\"\n",
            "1917    \"YOU ARE THE REAL @SSHO LE AND I HOPE PEOPLE O...\n",
            "2029    \"@Hoss \\xa0\\xa0@JenBroflovski\\xa0fuck you you ...\n",
            "2076               \"@caljb7 i'm on AAC then ya dumb fuck\"\n",
            "2105    \"@Enundr\\xa0lol.....You sir, are a jackass. wi...\n",
            "2456                   \"@FUCK:disqus\\xa0YOU CNN FAGGOTS.\"\n",
            "3079    \"@cnn-fcbba858f167b1594a66777bca:disqus \\n\\nYo...\n",
            "3100    \"@Bexxcc\\xa0\\xa0@bubzsucz\\n\\xa0YOU'RE THE SICK...\n",
            "3174    \"@lazerbyte Shut the fuck up -_- so where do y...\n",
            "3279    \"not funny jack@$$. you are a total db! friggi...\n",
            "3282    \"@RN506:disqus Read through the comments on th...\n",
            "3295    \"@ ede444 and that's all you've got to say is ...\n",
            "3306    \"Excellent command of the English language! On...\n",
            "3414         \"@bizora2:disqus you are a pathetic nobody.\"\n",
            "3677    \"@adamomars\\xa0\\xa0@Dieofnv\\xa0the very last t...\n",
            "3924    \"@Crissa:disqus LaRaza (The Race), NBP. Nation...\n",
            "Name: Comment, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Printing positive comments containing specific characters like '@' and 'https' to take as a sample\n",
        "print(positive_comments[positive_comments.str.contains('@|https', case=False)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlDMHcq5Vlrn",
        "outputId": "fc3eb515-1166-4894-c4eb-2448c8d986be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"@berethor099 Go ahead and try, dude. Go - fucking - ahead.\"\n"
          ]
        }
      ],
      "source": [
        "# We take a sample comment\n",
        "comment = positive_comments[1078]\n",
        "print(comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7BbW9crUCY1",
        "outputId": "3876c95a-953b-47ce-8896-9f3f03e7ffde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# We download the stopwords from Natural Language Toolkit\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkjsqiYxUhl1",
        "outputId": "9e29ec2f-7618-4a1f-ed11-7b2f50347347"
      },
      "outputs": [],
      "source": [
        "def strip_quotes(comment):\n",
        "    \"\"\"Remove quotes from the beginning and end of the comment.\"\"\"\n",
        "    return comment.strip('\"')\n",
        "\n",
        "def remove_hyperlinks(comment):\n",
        "    \"\"\"Remove hyperlinks from the comment.\"\"\"\n",
        "    return re.sub(r'https?://[^\\s\\n\\r]+', '', comment)\n",
        "\n",
        "def remove_hash_symbols(comment):\n",
        "    \"\"\"Remove hash # symbols from the comment.\"\"\"\n",
        "    return re.sub(r'#', '', comment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tTD6KiIYCXo"
      },
      "source": [
        "# Tokenization the string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtM17DnrYBSD",
        "outputId": "3907dccb-488f-499b-f428-59c409c23ad9"
      },
      "outputs": [],
      "source": [
        "def tokenize_comment(comment):\n",
        "    \"\"\"Tokenize the comment into words.\"\"\"\n",
        "    return word_tokenize(comment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuXmmGcSYma7"
      },
      "source": [
        "# Remove stop words and punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4B0K1XsYUEX",
        "outputId": "4672bd50-df31-4fe4-c183-dc859f961be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Due5IXoJZHUd"
      },
      "source": [
        "We might need to customize the stop words list for our applications. Since the model should diffrenciate between insults to users of the forum and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAbXF8FbY_4T",
        "outputId": "c9397c7f-df73-4371-fbd7-2d1a6a4ef4b6"
      },
      "outputs": [],
      "source": [
        "def clean_comment(comment_tokens, stopwords_english):\n",
        "    \"\"\"Remove stopwords and punctuation from the comment.\"\"\"\n",
        "    comment_clean = []\n",
        "    for word in comment_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "            word not in string.punctuation):\n",
        "            comment_clean.append(word)\n",
        "    return comment_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NUI0m1sZ31n"
      },
      "source": [
        "# Stemming\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKu7AJVMZ56N",
        "outputId": "504c4d19-03c4-48b5-8456-bb1f88701b04"
      },
      "outputs": [],
      "source": [
        "def stem_comment(comment_clean):\n",
        "    \"\"\"Stem the words in the comment.\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    comment_stem = [stemmer.stem(word) for word in comment_clean]\n",
        "    return comment_stem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remove Duplicate Words\n",
        "Our next step is to remove duplicate words from the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NitmF33ydqlc"
      },
      "outputs": [],
      "source": [
        "def remove_duplicates(comment_stem):\n",
        "    \"\"\"Remove duplicate words from the comment and return a list of unique words.\"\"\"\n",
        "    unique_words = set()\n",
        "    comment_NoDup = []\n",
        "    for word in comment_stem:\n",
        "        if word not in unique_words:\n",
        "            unique_words.add(word)\n",
        "            comment_NoDup.append(word)\n",
        "    return comment_NoDup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSr-_AQ-a23Y"
      },
      "source": [
        "# process_comment()\n",
        "Now we will create the process_comment() function that sums all the steps mentioned in the previous steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qyl6Hs7Wa93f"
      },
      "outputs": [],
      "source": [
        "def process_comment(comment, stopwords_english):\n",
        "    \"\"\"Process the comment through a series of functions and return the result.\"\"\"\n",
        "    comment1 = strip_quotes(comment)\n",
        "    comment2 = remove_hyperlinks(comment1)\n",
        "    comment3 = remove_hash_symbols(comment2)\n",
        "    comment_tokens = tokenize_comment(comment3)\n",
        "    comment_clean = clean_comment(comment_tokens, stopwords_english)\n",
        "    comment_stem = stem_comment(comment_clean)\n",
        "    comment_NoDup = remove_duplicates(comment_stem)\n",
        "    return comment_NoDup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaClO-XdlSE"
      },
      "source": [
        "Let's test our function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HklAWDlAdn7l",
        "outputId": "1b24b6f2-7caf-4481-8ba2-24d08f397dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m \"@ ede444 and that's all you've got to say is it well i won't be losing any sleep over it. Whether or not you are the same person or not you have the same stupid mentality. You're both like a couple of children so, easy to think you are one and the same. Your posts are the poorest in taste so you can't complain at mine. Calling me names just makes you look like a fool. If you think\\xa0I'm\\xa0showing off with any of my post it must mean you haven't done much with your own life. My posts are just a reflection of the truth to which you are obviously a green eyed monster. Nothing special to me or most people to say what languages one can speak or whether they've had the chance to meet important people.\\xa0Perhaps\\xa0you should concentrate on bettering your sad miserable life instead of posting childish comments to me.\"\n",
            "\u001b[94m\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\insult detetction\\Model-building.ipynb Cell 40\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92m\u001b[39m\u001b[39m'\u001b[39m, test_comment)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[94m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(process_comment(test_comment, stopwords_english))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(comment_NoDup,)\n",
            "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\insult detetction\\Model-building.ipynb Cell 40\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m comment2 \u001b[39m=\u001b[39m remove_hyperlinks(comment1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m comment3 \u001b[39m=\u001b[39m remove_hash_symbols(comment2)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m comment_tokens \u001b[39m=\u001b[39m tokenize_comment(comment3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m comment_clean \u001b[39m=\u001b[39m clean_comment(comment_tokens, stopwords_english)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m comment_stem \u001b[39m=\u001b[39m stem_comment(comment_clean)\n",
            "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\insult detetction\\Model-building.ipynb Cell 40\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_comment\u001b[39m(comment):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tokenize the comment into words.\"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenovo/Desktop/insult%20detetction/Model-building.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m word_tokenize(comment)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lenovo/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Pick a random test\n",
        "test_comment = positive_comments[3295]\n",
        "print('\\033[92m', test_comment)\n",
        "print('\\033[94m')\n",
        "\n",
        "print(process_comment(test_comment, stopwords_english))\n",
        "print(comment_NoDup,)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
